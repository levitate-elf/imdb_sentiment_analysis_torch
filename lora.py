import os
import sys
import logging
import datasets
import warnings
import time

warnings.filterwarnings("ignore")
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from transformers import (
    AutoModelForSequenceClassification, 
    DebertaV2Tokenizer, 
    DataCollatorWithPadding,
    get_linear_schedule_with_warmup
)
from peft import LoraConfig, get_peft_model, TaskType
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

print("ğŸš€ å¼€å§‹æç®€è®­ç»ƒæ–¹æ¡ˆ...")

# è¯»å–æ•°æ®
print("=== è¯»å–æ•°æ® ===")
train = pd.read_csv("./labeledTrainData.tsv", header=0, delimiter="\t", quoting=3)
test = pd.read_csv("./testData.tsv", header=0, delimiter="\t", quoting=3)

# æ•°æ®é¢„å¤„ç†
train, val = train_test_split(train, test_size=0.2, random_state=42)
train['sentiment'] = train['sentiment'].astype(int)
val['sentiment'] = val['sentiment'].astype(int)

print(f"æ•°æ®åŠ è½½å®Œæˆ - è®­ç»ƒé›†: {len(train)}, éªŒè¯é›†: {len(val)}")

# åŠ è½½æ¨¡å‹å’Œtokenizer
model_id = "microsoft/deberta-v3-base"
tokenizer = DebertaV2Tokenizer.from_pretrained(model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# åˆ›å»ºæ•°æ®é›†
def preprocess_function(examples):
    tokenized = tokenizer(
        examples['text'], 
        truncation=True, 
        max_length=256,
        padding=False
    )
    tokenized['labels'] = examples['label']
    return tokenized

train_dataset = datasets.Dataset.from_dict({
    'text': train['review'].tolist(),
    'label': train['sentiment'].tolist()
})
val_dataset = datasets.Dataset.from_dict({
    'text': val['review'].tolist(), 
    'label': val['sentiment'].tolist()
})

tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=['text'])
tokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=['text'])

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors="pt")

# åŠ è½½æ¨¡å‹
print("=== åŠ è½½æ¨¡å‹ ===")
model = AutoModelForSequenceClassification.from_pretrained(
    model_id,
    num_labels=2,
    torch_dtype=torch.float32,
)

# åº”ç”¨LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_CLS,
    target_modules=["query_proj", "value_proj", "key_proj"]
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ç§»åŠ¨åˆ°GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°: {device}")

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_dataloader = DataLoader(
    tokenized_train, 
    batch_size=16, 
    shuffle=True, 
    collate_fn=data_collator,
    num_workers=0
)
val_dataloader = DataLoader(
    tokenized_val, 
    batch_size=32, 
    collate_fn=data_collator,
    num_workers=0
)

# ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
total_steps = len(train_dataloader) * 1  # 1ä¸ªepoch
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(0.1 * total_steps),
    num_training_steps=total_steps
)

# è®­ç»ƒå‡½æ•°
def train_epoch(model, dataloader, optimizer, scheduler, device):
    model.train()
    total_loss = 0
    progress_steps = max(1, len(dataloader) // 10)  # æ¯10%è¿›åº¦æŠ¥å‘Šä¸€æ¬¡
    
    for step, batch in enumerate(dataloader):
        # ç§»åŠ¨åˆ°è®¾å¤‡
        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
        
        # å‰å‘ä¼ æ’­
        outputs = model(**batch)
        loss = outputs.loss
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        
        # è¿›åº¦æŠ¥å‘Š
        if step % progress_steps == 0:
            current_loss = loss.item()
            print(f"ğŸ“Š Step {step}/{len(dataloader)} - Loss: {current_loss:.4f}")
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1e9
                print(f"ğŸ’¾ GPUå†…å­˜: {memory_used:.2f}GB")
    
    return total_loss / len(dataloader)

# è¯„ä¼°å‡½æ•°
def evaluate(model, dataloader, device):
    model.eval()
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
            outputs = model(**batch)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=-1)
            
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(batch['labels'].cpu().numpy())
    
    accuracy = accuracy_score(true_labels, predictions)
    return accuracy

print("=" * 60)
print("ğŸ¯ å¼€å§‹æ‰‹åŠ¨è®­ç»ƒå¾ªç¯")
print("=" * 60)

# è®­ç»ƒå¾ªç¯
num_epochs = 3
for epoch in range(num_epochs):
    print(f"\nğŸ”¥ å¼€å§‹ç¬¬ {epoch+1}/{num_epochs} ä¸ªepoch")
    start_time = time.time()
    
    # è®­ç»ƒ
    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)
    
    # è¯„ä¼°
    val_accuracy = evaluate(model, val_dataloader, device)
    
    epoch_time = time.time() - start_time
    print(f"âœ… Epoch {epoch+1} å®Œæˆ - è€—æ—¶: {epoch_time:.2f}s")
    print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {train_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")

print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")

# é¢„æµ‹å‡½æ•°
def predict(model, dataset, tokenizer, device):
    model.eval()
    all_predictions = []
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_dataset = datasets.Dataset.from_dict({
        'text': test['review'].tolist()
    })
    
    tokenized_test = test_dataset.map(
        lambda examples: tokenizer(
            examples['text'], 
            truncation=True, 
            max_length=256,
            padding=False
        ),
        batched=True,
        remove_columns=['text']
    )
    
    test_dataloader = DataLoader(
        tokenized_test, 
        batch_size=32, 
        collate_fn=data_collator,
        num_workers=0
    )
    
    with torch.no_grad():
        for batch in test_dataloader:
            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
            outputs = model(**batch)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=-1)
            all_predictions.extend(preds.cpu().numpy())
    
    return all_predictions

print("\n=== å¼€å§‹é¢„æµ‹ ===")
test_predictions = predict(model, test, tokenizer, device)

# ä¿å­˜ç»“æœ
result_output = pd.DataFrame({
    "id": test["id"], 
    "sentiment": test_predictions
})
result_output.to_csv("./result/deberta_lora.csv", index=False)
print("âœ… é¢„æµ‹ç»“æœä¿å­˜æˆåŠŸ!")

print(f"é¢„æµ‹åˆ†å¸ƒ:\n{pd.Series(test_predictions).value_counts().sort_index()}")

print("=" * 60)
print("ğŸ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
print("=" * 60)