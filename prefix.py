import os
import sys
import logging
import datasets
import warnings
import time
warnings.filterwarnings("ignore")
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import (
    AutoModelForSequenceClassification, 
    DebertaV2Tokenizer, 
    DataCollatorWithPadding,
    get_linear_schedule_with_warmup
)
from peft import PrefixTuningConfig, get_peft_model, TaskType
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report

print("ğŸš€ å¼€å§‹DeBERTa Prefix Tuningè®­ç»ƒæ–¹æ¡ˆ...")

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True

set_seed(42)

# è¯»å–æ•°æ®
print("=== è¯»å–æ•°æ® ===")
train = pd.read_csv("./labeledTrainData.tsv", header=0, delimiter="\t", quoting=3)
test = pd.read_csv("./testData.tsv", header=0, delimiter="\t", quoting=3)

# æ•°æ®é¢„å¤„ç†
train, val = train_test_split(train, test_size=0.2, random_state=42)
train['sentiment'] = train['sentiment'].astype(int)
val['sentiment'] = val['sentiment'].astype(int)

print(f"æ•°æ®åŠ è½½å®Œæˆ - è®­ç»ƒé›†: {len(train)}, éªŒè¯é›†: {len(val)}")

# åŠ è½½æ¨¡å‹å’Œtokenizer
model_id = "microsoft/deberta-v3-base"
tokenizer = DebertaV2Tokenizer.from_pretrained(model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# åˆ›å»ºæ•°æ®é›†
def preprocess_function(examples):
    tokenized = tokenizer(
        examples['text'], 
        truncation=True, 
        max_length=256,
        padding=False
    )
    tokenized['labels'] = examples['label']
    return tokenized

train_dataset = datasets.Dataset.from_dict({
    'text': train['review'].tolist(),
    'label': train['sentiment'].tolist()
})
val_dataset = datasets.Dataset.from_dict({
    'text': val['review'].tolist(), 
    'label': val['sentiment'].tolist()
})

tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=['text'])
tokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=['text'])

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors="pt")

# åŠ è½½æ¨¡å‹
print("=== åŠ è½½æ¨¡å‹ ===")
model = AutoModelForSequenceClassification.from_pretrained(
    model_id,
    num_labels=2,
    torch_dtype=torch.float32,
)

# ä½¿ç”¨Prefix Tuningé…ç½®
peft_config = PrefixTuningConfig(
    task_type=TaskType.SEQ_CLS,
    num_virtual_tokens=20,
    encoder_hidden_size=768
)

# æ‰‹åŠ¨å®ç°Prefix Tuningçš„å‰å‘ä¼ æ’­
class CustomPrefixTuningModel(nn.Module):
    def __init__(self, model, peft_config):
        super().__init__()
        self.model = model
        self.peft_config = peft_config
        self.num_virtual_tokens = peft_config.num_virtual_tokens
        
        # åˆ›å»ºprefix embeddings
        self.prefix_embeddings = nn.Embedding(
            self.num_virtual_tokens, 
            self.model.config.hidden_size
        )
        
    def forward(self, input_ids, attention_mask=None, labels=None):
        batch_size = input_ids.shape[0]
        
        # åˆ›å»ºprefix tokens
        prefix_tokens = torch.arange(self.num_virtual_tokens).repeat(batch_size, 1).to(input_ids.device)
        prefix_embeds = self.prefix_embeddings(prefix_tokens)
        
        # è·å–åŸå§‹è¾“å…¥çš„embeddings
        inputs_embeds = self.model.deberta.embeddings(input_ids)
        
        # æ‹¼æ¥prefixå’ŒåŸå§‹è¾“å…¥
        combined_embeds = torch.cat([prefix_embeds, inputs_embeds], dim=1)
        
        # è°ƒæ•´attention mask
        if attention_mask is not None:
            prefix_mask = torch.ones(batch_size, self.num_virtual_tokens).to(attention_mask.device)
            combined_mask = torch.cat([prefix_mask, attention_mask], dim=1)
        else:
            combined_mask = None
        
        # é€šè¿‡æ¨¡å‹
        outputs = self.model(
            inputs_embeds=combined_embeds,
            attention_mask=combined_mask,
            labels=labels
        )
        
        return outputs

# åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹
custom_model = CustomPrefixTuningModel(model, peft_config)
custom_model.to(model.device)

# åªè®­ç»ƒprefix embeddings
for name, param in custom_model.named_parameters():
    if 'prefix_embeddings' in name:
        param.requires_grad = True
        print(f"è®­ç»ƒå‚æ•°: {name}")
    else:
        param.requires_grad = False

# è®¡ç®—å¯è®­ç»ƒå‚æ•°
trainable_params = sum(p.numel() for p in custom_model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in custom_model.parameters())
print(f"trainable params: {trainable_params} || all params: {total_params} || trainable%: {100 * trainable_params / total_params:.4f}")

# ç§»åŠ¨åˆ°GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
custom_model.to(device)
print(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°: {device}")

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_dataloader = DataLoader(
    tokenized_train, 
    batch_size=8,
    shuffle=True, 
    collate_fn=data_collator,
    num_workers=0
)
val_dataloader = DataLoader(
    tokenized_val, 
    batch_size=16,
    collate_fn=data_collator,
    num_workers=0
)

# ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
optimizer = torch.optim.AdamW(
    custom_model.parameters(), 
    lr=1e-3,
    weight_decay=0.01
)

total_steps = len(train_dataloader) * 3
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(0.1 * total_steps),
    num_training_steps=total_steps
)

# è®­ç»ƒå‡½æ•°
def train_epoch(model, dataloader, optimizer, scheduler, device):
    model.train()
    total_loss = 0
    progress_steps = max(1, len(dataloader) // 10)
    
    for step, batch in enumerate(dataloader):
        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
        
        try:
            # å‰å‘ä¼ æ’­
            outputs = model(
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask'],
                labels=batch['labels']
            )
            loss = outputs.loss
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
            
            # è¿›åº¦æŠ¥å‘Š
            if step % progress_steps == 0:
                current_loss = loss.item()
                print(f"ğŸ“Š Step {step}/{len(dataloader)} - Loss: {current_loss:.4f}")
                if torch.cuda.is_available():
                    memory_used = torch.cuda.memory_allocated() / 1e9
                    print(f"ğŸ’¾ GPUå†…å­˜: {memory_used:.2f}GB")
                    
        except RuntimeError as e:
            if "out of memory" in str(e):
                print("ğŸ’¥ å†…å­˜ä¸è¶³ï¼Œè·³è¿‡è¯¥batch")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                continue
            else:
                raise e
    
    return total_loss / len(dataloader)

# è¯„ä¼°å‡½æ•°
def evaluate(model, dataloader, device):
    model.eval()
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
            try:
                outputs = model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask']
                )
                logits = outputs.logits
                preds = torch.argmax(logits, dim=-1)
                
                predictions.extend(preds.cpu().numpy())
                true_labels.extend(batch['labels'].cpu().numpy())
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print("ğŸ’¥ è¯„ä¼°æ—¶å†…å­˜ä¸è¶³ï¼Œè·³è¿‡è¯¥batch")
                    continue
                else:
                    raise e
    
    accuracy = accuracy_score(true_labels, predictions)
    return accuracy

print("=" * 60)
print("ğŸ¯ å¼€å§‹è‡ªå®šä¹‰Prefix Tuningè®­ç»ƒå¾ªç¯")
print("=" * 60)

# è®­ç»ƒå¾ªç¯
num_epochs = 3
for epoch in range(num_epochs):
    print(f"\nğŸ”¥ å¼€å§‹ç¬¬ {epoch+1}/{num_epochs} ä¸ªepoch")
    start_time = time.time()
    
    # è®­ç»ƒ
    train_loss = train_epoch(custom_model, train_dataloader, optimizer, scheduler, device)
    
    # è¯„ä¼°
    val_accuracy = evaluate(custom_model, val_dataloader, device)
    
    epoch_time = time.time() - start_time
    print(f"âœ… Epoch {epoch+1} å®Œæˆ - è€—æ—¶: {epoch_time:.2f}s")
    print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {train_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")

print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")

# é¢„æµ‹å‡½æ•°
def predict(model, test_data, tokenizer, device):
    model.eval()
    all_predictions = []
    
    test_dataset = datasets.Dataset.from_dict({
        'text': test_data['review'].tolist()
    })
    
    tokenized_test = test_dataset.map(
        lambda examples: tokenizer(
            examples['text'], 
            truncation=True, 
            max_length=256,
            padding=False
        ),
        batched=True,
        remove_columns=['text']
    )
    
    test_dataloader = DataLoader(
        tokenized_test, 
        batch_size=16,
        collate_fn=data_collator,
        num_workers=0
    )
    
    with torch.no_grad():
        for batch in test_dataloader:
            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
            try:
                outputs = model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask']
                )
                logits = outputs.logits
                preds = torch.argmax(logits, dim=-1)
                all_predictions.extend(preds.cpu().numpy())
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print("ğŸ’¥ é¢„æµ‹æ—¶å†…å­˜ä¸è¶³ï¼Œè·³è¿‡è¯¥batch")
                    continue
                else:
                    raise e
    
    return all_predictions

print("\n=== å¼€å§‹é¢„æµ‹ ===")
test_predictions = predict(custom_model, test, tokenizer, device)

# ä¿å­˜ç»“æœ
result_output = pd.DataFrame({
    "id": test["id"], 
    "sentiment": test_predictions
})
result_output.to_csv("./result/deberta_prefix.csv", index=False)
print("âœ… é¢„æµ‹ç»“æœä¿å­˜æˆåŠŸ!")

print(f"é¢„æµ‹åˆ†å¸ƒ:\n{pd.Series(test_predictions).value_counts().sort_index()}")

print("=" * 60)
print("ğŸ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
print("=" * 60)